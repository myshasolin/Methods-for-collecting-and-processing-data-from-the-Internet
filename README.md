<center>
  <h2>
    <a href="https://github.com/myshasolin/recommender_systems">
      здесь упражнения и самостоятельные проекты, выполненные в рамках курса "Методы сбора и обработки данных из сети Интернет", от простого (1) к сложному (9):
    </a>
  </h1>
</center>

<table style="border: 2px double;">
  <tr>
    <th></th>
    <th>тема</th>
    <th>описание задания</th>
    <th>решение</th>
    <th>библиотеки</th>
  </tr>
  <tr>
    <td>
      1
    </td>
    <td>
      <a href="#">
        Основы клиент-серверного взаимодействия. Работа с API
      </a>
    </td>
    <td>
      1. Посмотреть документацию к API GitHub, вывести список репозиториев для конкретного пользователя, сохранить JSON-вывод в файле *.json<br>2. Работа с недокументированным API. Нужно ввести релевантный запрос на сайте <a href="https://www.delivery-club.ru/search">delivery-club</a><br>(а) из предложенных точек с помощью API найти долю (в %) с бесплатной и платной доставкой. Для каждой категории рассчитать среднюю минимальную стоимость заказа<br>(б) для каждой из категорий из пункта (а) рассчитать долю (в %) магазинов и ресторанов
    </td>
    <td>
      во втором задании местами код дублируется, но это исключительно в образовательных целях, чтоб показать, что и где выводится
    </td>
    <td>
      numpy, pandas, json, requests, pprint
    </td>
  </tr>
  <tr>
    <td>
      2
    </td>
    <td>
      <a href="#">
        Парсинг данных. HTML, DOM, XPath
      </a>
    </td>
    <td>
      Написать приложение или функцию, которые собирают основные новости с сайта на выбор <a href="https://lenta.ru/">lenta.ru</a>, <a href="https://dzen.ru/news">Yandex новости</a>. Для парсинга использовать XPath<br>Структура данных в виде словаря должна содержать:<br>- название источника;<br>- наименование новости;<br>- ссылку на новость;<br>- дата публикации.
    </td>
    <td>
      написал 2 функции на 2 сайта, они обе лежат в одном файле. Подробно описание функций оставил в самом файле в markdown перед кодом
    </td>
    <td>
      requests, lxml, json, pprint
  </tr>
  <tr>
    <td>
      3
    </td>
    <td>
      <a href="#">
        Парсинг данных. HTML, Beautiful Soap
      </a>
    </td>
    <td>
      Собрать информацию о вакансиях на вводимую должность с сайтов <a href="https://hh.ru/">hh.ru</a> и/или <a href="https://www.superjob.ru/">superjob.ru</a> и/или <a href="https://www.rabota.ru/">rabota.ru</a><br>Приложение должно анализировать несколько страниц сайта. Получившийся список должен содержать в себе минимум:<br>- Наименование вакансии<br>- Предлагаемую зарплату (дополнительно: разносим в три поля: минимальная и максимальная и валюта. цифры преобразуем к цифрам)<br>- Ссылку на саму вакансию<br>- Сайт, откуда собрана вакансия<br>- Можно добавить ещё параметры вакансии (например, работодателя и расположение).<br>Структура должна быть одинаковая для вакансий с всех сайтов. Общий результат можно вывести с помощью dataFrame через pandas, сохранить в json, либо csv.
    </td>
    <td>
      Весь код завернул в одну функцию, на выбор можно работать с тремя сайтами, проверил, все работают) Описание оставил в markdown там же в файле. sleep() выкрутил на 5 секунд.<br>SuperJob в Colab часто ругается и блочит, недружелюбный он. Локально с ним таких проблеем нет.<br>JSON-файлы, в т.ч. и от SuperJob, в Pull Requests тоже добавил
    </td>
    <td>
      bs4, requests, re, time, json, pprint
  </tr>
  <tr>
    <td>
      4
    </td>
    <td>
      <a href="#">
        Система управления базами данных MongoDB в Python
      </a>
    </td>
    <td>
      1. Развернуть у себя на компьютере/виртуальной машине/хостинге MongoDB и реализовать функцию, которая будет добавлять только новые вакансии/продукты в вашу базу<br>2. Написать функцию, которая производит поиск и выводит на экран вакансии с заработной платой больше введённой суммы (необходимо анализировать оба поля зарплаты).<br> 3. Любая аналитика. Например matching ваканский с разных площадок
    </td>
    <td>
      Основой для задания взял код за 3-ю тему, на этот раз разделив функцию на три (для hh.ru, superjob.ru и rabota.ru). Подробные комментарии по заданиям оставил в markdown в самом файле, а в конце файла построил диаграмм пучок. В самом скрипте для его отработки "сверху вниз" в начале удаляю базу, потом создаю.<br>P.S.: Хедхантер иногда может собирать чуть больше вакансий, например, не 2000, а 2005. Дополнительные 5 – это тоже ссылки на вакансии. Это проплаченные объявления, что периодически появляются на страницах поиска самых популярных направлений работы (например, продавец-консультант, как у меня в файле, курьер, водитель погрузчика и т.п.). Если такие объявления появляются, то разово на странице становится не 40 вакансий, а 41-42, вот они и цепляются.
    </td>
    <td>
      bs4, requests, re, time, json, pprint, pymongo, numpy, matplotlib
  </tr>
  <tr>
    <td>
      5
    </td>
    <td>
      <a href="#">
        Парсинг данных. Scrapy
      </a>
    </td>
    <td>
      I вариант<br>1) Написать паука, чтобы он формировал item по структуре:<br>-Наименование вакансии<br>-Зарплата от<br>-Зарплата до<br>-Ссылку на саму вакансию<br>И складывал все записи в БД(любую)<br>2) Создать в имеющемся проекте второго паука по сбору вакансий с сайта superjob. Паук должен формировать item'ы по аналогичной структуре и складывать данные также в БД<br><br>II вариант<br>1) Создать пауков по сбору данных о книгах с сайтов labirint.ru и/или book24.ru<br>2) Каждый паук должен собирать:<br>-Ссылку на книгу<br>-Наименование книги<br>-Автор(ы)<br>-Основную цену<br>-Цену со скидкой<>-Рейтинг книги<br>3) Собранная информация должна складываться в базу данных
    </td>
    <td>
      Сделал оба варианта задания, они лежат, соответственно, в двух папках, каждый вариант решения в своей одноимённой. Там же рядом с ними лежат собранные JSON-файлы с данными, пять пауков – пять JSON-коллекций<br>Там же к ним рядом положил блокнот, в котором подробно расписал, что я делал и что из всего этого получилось)
    </td>
    <td>
      scrapy, itemadapter, pymongo, twisted.internet, jobparser, re
  </tr>
  <tr>
    <td>
      6
    </td>
    <td>
      <a href="#">
        Фреймворк Scrapy, pipelines, Splash
      </a>
    </td>
    <td>
      1) Взять любую категорию товаров на сайте <a href="https://www.castorama.ru/">Castorama</a>, собрать следующие данные:<br>-название<br>-все фото<br>-ссылка<br>-цена<br>Реализовать очистку и преобразование данных с помощью ItemLoader. Цены должны быть в виде числового значения.<br>Дополнительно:<br>2)Написать универсальный обработчик характеристик товаров, который будет формировать данные вне зависимости от их типа и количества.<br>3)Реализовать хранение скачиваемых файлов в отдельных папках, каждая из которых должна соответствовать собираемому товару
    </td>
    <td>
      Описание оставил в .ipynb-файле (так совсем коротенько и по делу), который вместе с собранной базой положил рядом с проектом
    </td>
    <td>
      scrapy, itemloaders, itemadapter, pymongo, hashlib, twisted, sys
  </tr>
  <tr>
    <td>
      7
    </td>
    <td>
      <a href="#">
        Парсинг данных. Selenium в Python
      </a>
    </td>
    <td>
      Вариант I. Написать программу, которая собирает входящие письма из своего или тестового почтового ящика и сложить данные о письмах в базу данных (от кого, дата отправки, тема письма, ссылка)<br>Вариант II. Любая работа с JS на сайте со сбором данных
    </td>
    <td>
      Сделал оба варианта, коротенькое описание, что и зачем делал, оставил в .ipynb-файле
    </td>
    <td>
      selenium, time, pymongo, os, dotenv
  </tr>
  <tr>
    <td>
      8
    </td>
    <td>
      <a href="#">
        Фреймворк Scrapy. Реализация механизмов клиент-серверного взаимодействия
      </a>
    </td>
    <td>
       Зайти на любой сайт с аутентификацией и где можно собрать какую-нибудь информацию и эту информацию собрать
    </td>
    <td>
      Написал паука, который под моим логином собрал всю информацию обо всех товарах на сайте <a href="https://www.galser.ru/">Галсэра</a> за 1 час (файл логов со временем сбора коллекций в проекте есть). И это очень крутой и впечатляющий результат. Что я собрал:<br>- category — категория товаров. Категорию же сделал названием коллекций, так как товары очень разношёрстные<br>- article — международный артикул товара — он же ключ «_id» в базе, что помогает избегать дублей<br>- name — название товара<br>- recommended_retail_price — РРЦ - рекомендованная розничная цена<br>- link — ссылка на товар<br>- description — описание<br>- product_characteristics_keys — полная таблица характеристик<br>Всю инфу складываю в БД GALSER. Дублями, если они встретятся при повторном парсинге, обновляю информацию в базе. Для примера несколько собранных коллекций по коммерчески интересным для меня категориям положил в папку рядом с проектом.<br>Работа под логином и паролем позволяет собирать не только общедоступную информацию, но и цену закупки и фактические товарные остатки. В работе я этого делать не стал, так как информация коммерческая
    </td>
    <td>
      scrapy, itemloaders, itemadapter, pymongo, twisted, urllib, dotenv, re, os, copy, datetime
  </tr>
  <tr>
    <td>
      9
    </td>
    <td>
      <a href="#">
        Библиотека scrapy-selenuin
      </a>
    </td>
    <td>
       Дополнительно
    </td>
    <td>
      Изучал самостоятельно. Эта библиотека позволяет обойти ограничение в 50 вакансий со страницы, так как позволяет подстроиться под динамический адаптив JavaScript
    </td>
    <td>
      scrapy, itemloaders, itemadapter, importlib, selenium, scrapy_selenium, pymongo, re, os, sys, csv, twisted
  </tr>
  <tr>
    <td>
      10
    </td>
    <td>
      <a href="#">
        Дополнительно - сбор информации из Telegram
      </a>
    </td>
    <td>
       Дополнительно
    </td>
    <td>
      Изучал самостоятельно. Парсер довольно простой, т.к. Telegram имеет api. В закомментированных строках показываю как писать сообщения, скачивать медиа, получить список участников выбранной группы
    </td>
    <td>
      telethon, dotenv, time, os
  </tr>
</table>
